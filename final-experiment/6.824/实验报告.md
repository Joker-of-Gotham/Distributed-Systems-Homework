## MIT 6.824 分布式系统实验报告
## 题目：容错的 Key/Value 键值存储系统（Lab 2B → Lab 2C → Lab 3A）

---

### 封面信息（提交前务必填写）

| 项目 | 内容 |
|---|---|
| 组长 | （填写：姓名） |
| 成员 | （填写：姓名1、姓名2；每队最多 3 人） |
| 工作量分配比例 | 组长：__% ；成员1：__% ；成员2：__%（不写视为同等贡献） |
| 实验范围 | Lab 2B：日志复制与一致性；Lab 2C：状态信息持久化；Lab 3A：基于 Raft 的容错 KV 服务 |
| **范围声明** | **本次实验不实现 Lab 2D（日志压缩/快照）语义**，代码仅保证 2A/2B/2C + 3A 测试通过与性能达标 |
| 仓库路径 | `~/distributed-sys/final-experiment/6.824/src` |
| 实验环境 | Linux (WSL2)；Go `go1.15.15 linux/amd64` |
| 报告日期 | 2025-12-29 |
| 截止时间 | 2026-01-18 23:59（晚交 1 天扣 1 分） |

---

### 分工说明（建议填写，便于答辩/验收）

| 模块 | 负责人 | 主要工作内容（示例） | 比例 |
|---|---|---|---:|
| Lab 2B | （填写） | AppendEntries 日志复制、冲突回退优化、commit/apply | __% |
| Lab 2C | （填写） | persist/readPersist、崩溃恢复、Figure 8 场景修复 | __% |
| Lab 3A | （填写） | KVServer/Clerk、幂等去重、leader 校验、线性一致性 | __% |
| 测试与性能 | （填写） | 性能调参、全量回归、输出整理 | __% |
| 报告撰写 | （填写） | 报告结构化、图表与复盘 | __% |

### 摘要

本实验以 MIT 6.824 Raft 为基础，完成了 **Lab 2B（日志复制与一致性）**、**Lab 2C（持久化）** 与 **Lab 3A（容错 Key/Value 服务）**。在 Raft 层，我们实现了 AppendEntries 日志复制、leader/follower 日志一致性修复、提交规则（含 Figure 8 关键约束）、以及基于 persister 的状态持久化与重启恢复；在 KV 层，我们将客户端 Get/Put/Append 操作封装为 Raft log 的 command，通过 applyCh 回放驱动状态机更新，并在客户端重试与网络不可靠条件下，通过 **请求去重（幂等）+ leader 校验** 保证线性一致性与“至多一次”语义。最终性能达到 A 档：**2A+2B+2C 用时 166.524s（<170s）**，**3A 用时 235.993s（<250s）**。

### 关键词

Raft；日志复制；一致性；持久化；线性一致性；幂等；容错；RPC；并发控制

---

### 目录（建议按章节阅读）

- 1. 实验总览与技术路线
- 2. Lab 2B：日志复制与一致性
- 3. Lab 2C：状态信息持久化
- 4. Lab 3A：基于 Raft 的容错 Key/Value 服务
- 5. 测试结果与性能评估
- 6. 个人总结（困难、解决方案、心得体会）
- 附录：复现实验与代码结构索引

---

## 1. 实验总览与技术路线

### 1.1 任务分析（从“题目要求”到“可落地实现”）

本次题目关注的是“容错 Key/Value 键值存储系统”，其技术路线按依赖关系自底向上分为三层：

1. **Raft（Lab 2B）**：实现日志复制与一致性，保证多数派存活时复制状态机可继续推进。
2. **Raft 持久化（Lab 2C）**：实现崩溃重启后的状态恢复，保证安全性不被破坏、服务可继续提供。
3. **KV 服务（Lab 3A）**：把 KV 操作作为 Raft command 复制，解决 Client↔Server 不可靠 RPC 下的重试与幂等，最终对外提供线性一致接口。

评分点与我们的交付对齐：

- **基础功能（30%）**：2A/2B/2C + 3A 全测通过。
- **一致性（20%）**：Raft safety + KV 线性一致（通过 porcupine 检验）。
- **容错方案（15%）**：支持丢包、延迟、分区、重启。
- **高并发性能（15%）**：2A+2B+2C < 170s；3A < 250s（见第 5 章数据）。
- **代码干净（重要扣分项）**：不实现 2D 语义，避免提交快照/日志压缩实现导致扣分。

### 1.2 系统整体架构（从论文到工程落地）

```
+----------------------+         +----------------------+         +----------------------+
|      Client          |  RPC    |      KV Server       | command |        Raft          |
| (Clerk: Get/Put/App) +-------->+ (KVServer: handler)  +-------->+ (consensus + log)    |
+----------------------+         +----------+-----------+         +----------+-----------+
                                           |                                |
                                           | ApplyMsg                        |
                                           v                                |
                                   +---------------+                        |
                                   |  StateMachine |<-----------------------+
                                   |   KV Store    |
                                   +---------------+
```

### 1.3 一次请求的“理论到实践全流程”（题目给的流程扩展版）

以 `Client.Put("x","1")` 为例：

1. **Client** 通过 RPC 发送 `Put(x,1)` 到它认为的 leader 服务器（若失败会重试并切换 server）。
2. **KVServer** 收到请求后，先做必要的重复检测/快速失败判断，然后把请求封装为 `Op{Type:"Put", Key:"x", Value:"1", ClientId, RequestId}`。
3. **KVServer → Raft** 调用 `rf.Start(op)`：  
   - 若本机不是 leader，立刻返回 `ErrWrongLeader`，Client 重试；  
   - 若是 leader，将 op 追加为新的 log entry，开始并行复制给 follower。
4. **Raft 层** 通过 AppendEntries 复制日志；当该 entry 在多数派上复制成功后，leader 推进 `commitIndex`。
5. **Raft → KVServer**：applier goroutine 把已提交 entry 作为 `ApplyMsg{CommandValid:true, Command:op, CommandIndex:i}` 发送到 `applyCh`。
6. **KVServer** 从 `applyCh` 读到 ApplyMsg 后：
   - 按序应用到状态机（KV map），并更新去重表；
   - 唤醒等待该 index 的 RPC handler（若此时已不是 leader/term 变化，则返回 ErrWrongLeader）。
7. **KVServer → Client** 返回 OK。Client 完成一次线性一致的 Put。

> 关键点：**线性一致性依赖于“对外成功返回”的操作一定已经被多数派 commit 并应用**，并且同一 key 的操作顺序由 Raft log 全序决定。

### 1.4 代码结构（便于读者快速定位）

- `src/raft/raft.go`：Raft 核心（选举、日志复制、提交、apply、持久化）
- `src/raft/persister.go`：持久化读写（lab 提供接口，2C 使用）
- `src/kvraft/server.go`：KVServer（RPC、与 Raft 交互、apply、去重）
- `src/kvraft/client.go`：Clerk（重试、leader 缓存、请求编号）
- `src/kvraft/common.go`：RPC 参数/返回值与错误码

---

## 2. Lab 2B：日志复制与一致性

> 本章目标：实现 Raft 的 **日志复制** 与 **日志一致性修复**，确保所有副本对 committed log 达成一致并按序 apply。

### 2.1 任务分析（需要解决的关键问题）

Lab 2B 的难点集中在三个“必须同时成立”的性质：

1. **Log Matching**：如果两台机器的某个索引 i 的 term 相同，则 i 之前的所有日志项完全一致。
2. **Leader Completeness**：已提交的日志项必须出现在之后任期的 leader 日志中。
3. **State Machine Safety**：任何服务器在 index i apply 的 command 必须和其它服务器在 i apply 的 command 相同。

从实现角度，必须补齐：

- `Start()`：leader 接收新命令并追加到日志。
- `AppendEntries` RPC：心跳 + 日志复制。
- 日志一致性检查与冲突修复（含快速回退优化）。
- `commitIndex` 推进与 apply 通道。

#### 2.1.1（补充）2A 选举与心跳：2B/2C 的前置地基

虽然本报告重点按要求叙述 2B→2C→3A，但实际系统要能跑通必须先满足 2A（选举与心跳）。在工程实现中我们遵循以下原则：

- **角色状态机**：Follower ↔ Candidate ↔ Leader；所有状态变化都以 `currentTerm` 为边界。
- **选举超时随机化**：每个节点使用随机选举超时（`200ms~350ms`）避免同时发起选举导致 split vote。
- **心跳约束**：leader 每 `100ms` 发送一次心跳/AppendEntries（每秒 10 次，满足测试对心跳频率的上限）。
- **投票限制（论文 5.4.1）**：RequestVote 中比较 `(LastLogTerm, LastLogIndex)`，确保新 leader 的日志“不落后”，从而维护 Leader Completeness。

### 2.2 功能设计（数据结构与核心状态）

#### 2.2.1 日志条目（LogEntry）

文件：`src/raft/raft.go`

```go
type LogEntry struct {
    Term    int
    Command interface{}
}
```

#### 2.2.2 leader 复制状态（nextIndex / matchIndex）

leader 维护每个 follower 的复制进度：

- `nextIndex[i]`：下次要发给 follower i 的日志索引
- `matchIndex[i]`：follower i 已知复制成功的最大索引

它们的作用是把“全量同步”降为“增量复制 + 回退修复”。

#### 2.2.3 AppendEntries RPC（带快速回退字段）

请求：

```go
type AppendEntriesArgs struct {
    Term         int
    LeaderId     int
    PrevLogIndex int
    PrevLogTerm  int
    Entries      []LogEntry
    LeaderCommit int
}
```

响应（关键：XTerm/XIndex/XLen，用于快速回退）：

```go
type AppendEntriesReply struct {
    Term    int
    Success bool
    XTerm   int
    XIndex  int
    XLen    int
}
```

### 2.3 关键流程实现（从“论文规则”到“代码路径”）

#### 2.3.1 Start()：leader 追加日志并触发复制

设计要点：

- **只允许 leader Start**
- **先本地追加 + persist，再并发复制**（保证 2C 的持久化安全）
- **立即广播**（不等心跳 tick），缩短提交延迟，提升 2B/3A 性能

#### 2.3.2 follower 端 AppendEntries：一致性检查 + 冲突修复

AppendEntries handler 的核心是“两步检查”：

1. `PrevLogIndex` 是否存在（不存在→日志过短）
2. `PrevLogTerm` 是否匹配（不匹配→日志冲突）

然后才可以：

- 删除冲突日志（从第一个冲突点开始截断）
- 追加 leader 的新 entries
- 根据 `LeaderCommit` 更新本地 commitIndex

#### 2.3.3 leader 端冲突回退：fast backup（2C 也依赖它）

当 follower 回复 `Success=false` 时，leader 按 reply 信息快速调整 nextIndex：

- `XTerm==-1`：说明 follower 日志过短，`nextIndex = XIndex`（直接跳）
- 否则：尝试在 leader 自己日志里找 `XTerm`
  - 找到则 `nextIndex = lastIndexOf(XTerm)+1`
  - 找不到则 `nextIndex = XIndex`

这样能把“逐条回退”优化为“按 term 快速跳跃”，显著降低 Figure 8 与 churn 场景下的 RPC 次数与耗时。

#### 2.3.4 关键函数与代码路径索引（便于从报告落地到源码）

Raft 的 2B/2C 关键代码路径可按“事件驱动”理解：

- **选举相关**
  - `ticker()`：超时触发选举
  - `startElection()`：转 Candidate、递增 term、并发 RequestVote
  - `RequestVote()`：按 term 与日志新旧（5.4.1）决定是否投票
  - `becomeLeader()`：初始化 nextIndex/matchIndex 并立刻发送心跳
- **复制相关**
  - `Start(command)`：追加日志并触发复制
  - `broadcastAppendEntries()`：并发向所有 follower 发送 AppendEntries
  - `sendAppendEntriesToPeer()`：构造 PrevLogIndex/PrevLogTerm 与 entries，RPC 后根据 reply 更新 nextIndex/matchIndex
  - `AppendEntries()`：follower 侧一致性检查与冲突截断/追加
- **提交与 apply**
  - `advanceCommitIndex()`：按 matchIndex 多数派推进 commitIndex（含 Figure 8 约束）
  - `applier()`：通过 applyCond 等待 commitIndex 推进，按序向 applyCh 输出 ApplyMsg

在实现时一个最重要的工程纪律是：

> **绝不在持有 rf.mu 的情况下进行 RPC**（避免阻塞与死锁）。因此我们会“锁内构造参数/拷贝必要字段 → 释放锁 → RPC → 重新加锁处理回复”。

### 2.4 提交规则（commitIndex）与 Figure 8 关键约束

**核心约束**（Raft 论文的 Figure 8 场景）：

> leader 只能在“多数派复制成功”的基础上提交条目，但为了安全性，leader 只能直接推进 commitIndex 到 **当前任期**的条目；旧任期条目会随着提交一个当前任期的新条目而被“间接提交”。

工程落地：

- 在 `advanceCommitIndex()` 中额外检查 `log[n].Term == currentTerm`
- 这条检查是 2C 的 Figure 8 测试是否能稳定通过的关键

### 2.5 apply 机制与并发控制（避免忙等、避免乱序）

为保证 apply 顺序与性能：

- 使用 `sync.Cond`（`applyCond`）在 `commitIndex` 推进时唤醒 applier
- applier 线程只在 `lastApplied < commitIndex` 时继续推进
- 每条 ApplyMsg 都携带 `CommandIndex`，上层 KVServer 可以据此做索引对齐与等待唤醒

### 2.6 容错分析（2B 必须覆盖的故障）

需要明确系统在不可靠网络下的行为：

- **RPC 丢失/延迟**：leader 继续心跳和重试；follower 以 term/prevLogIndex/prevLogTerm 为准判定是否接受。
- **网络分区**：少数派无法提交（commitIndex 不会推进），恢复后通过冲突回退补齐日志。
- **leader 崩溃**：新 leader 选举产生；通过 2B 的日志新旧对比+ 2C 的持久化保证安全性。

### 2.7 性能策略（直接对应评分 A 档）

1. **心跳频率**：`heartbeatInterval = 100ms`（每秒 10 次，满足 tester 上限）
2. **选举超时随机化**：`200ms~350ms`，减少 split vote，同时确保 5 秒内大概率完成选举
3. **立即广播复制**：在 `Start()` 与 `becomeLeader()` 后立刻 `broadcastAppendEntries()`
4. **fast backup**：减少 2C Figure 8 / churn 场景下的回退 RPC 数

---

## 3. Lab 2C：状态信息持久化

> 本章目标：节点崩溃并重启后，Raft 能恢复到崩溃前的安全状态，继续参与选举与复制，不破坏一致性。

### 3.1 任务分析（2C 的“必须持久化”状态）

论文 Figure 2 指定必须持久化的状态：

- `currentTerm`：保证任期单调不减（防止回到旧 term 接收旧 leader）
- `votedFor`：保证同一任期最多投一票
- `log[]`：保证崩溃后不丢已接收的日志（尤其是 leader 本地追加后必须可恢复）

### 3.2 功能设计（persist/readPersist 的工程落地）

#### 3.2.1 persist()：序列化并写入 persister

我们使用 labgob 编码（与 MIT 框架兼容）：

```go
e.Encode(rf.currentTerm)
e.Encode(rf.votedFor)
e.Encode(rf.log)
rf.persister.SaveRaftState(data)
```

#### 3.2.2 readPersist()：重启恢复

在 `Make()` 中执行：

1. 初始化默认值（dummy log、term=0、votedFor=-1）
2. 调用 `readPersist(persister.ReadRaftState())` 覆盖恢复
3. 启动 ticker/heartbeat/applier goroutine

### 3.3 持久化调用点（保证“先落盘再对外承诺”）

关键原则：**只要持久化字段发生变化，就必须 persist**，典型触发点：

- 任期变化：收到更大 term 的 RPC、开始新选举
- 投票变化：投票给某候选人
- 日志变化：追加 entries、截断冲突日志

这一点直接决定 2C “crash/restart” 与 “Figure 8” 的稳定性。

为了让“理论 → 代码”映射更明确，下面给出一个持久化触发点清单（读者可直接在源码中搜索这些位置）：

| 场景 | 会修改的持久化字段 | 必须 persist 的原因 | 典型代码位置（示例） |
|---|---|---|---|
| 收到更大 term 的 RPC | currentTerm、votedFor | 防止回退到旧 term | `RequestVote()` / `AppendEntries()` |
| 开始新选举 | currentTerm、votedFor | term 递增 + 给自己投票 | `startElection()` |
| 给候选人投票 | votedFor | 同任期只能投一次 | `RequestVote()` |
| leader 追加新日志 | log | 防止 leader 崩溃后丢已接收命令 | `Start()` |
| follower 截断/追加日志 | log | 保证崩溃后仍保持一致性修复结果 | `AppendEntries()` |

### 3.4 容错分析（2C 的 crash-restart 场景）

需要覆盖的典型场景：

- **leader 崩溃重启**：通过持久化 term/log，重启后不会继续以旧 term 自称 leader，也不会丢失已接收日志。
- **follower 崩溃重启**：恢复 log 后可以继续被 leader 追赶（fast backup 依然有效）。
- **网络分区 + 崩溃**：恢复后通过 AppendEntries 冲突修复对齐到 leader。

### 3.5 与 3A 的关系（为什么 2C 是 KV 的地基）

3A 要求的“容错 KV”本质上依赖：

- Raft 崩溃重启后不会破坏 log 的全序；
- KVServer 只需要“顺序 apply log entry”，就能恢复到一致状态；
- 因此 2C 是 3A 的必要前置条件。

---

## 4. Lab 3A：基于 Raft 的容错 Key/Value 服务

> 本章目标：对外提供线性一致的 Get/Put/Append；在 RPC 丢失、分区、重启等条件下仍正确；并通过去重确保幂等。

### 4.1 任务分析（KV 层比 Raft 更容易出错的点）

Raft 解决的是“复制与一致顺序”，但 3A 还必须解决：

1. **Client↔Server RPC 丢失/超时**：Client 重试会导致重复请求。
2. **leader 变化**：Server 可能在处理过程中失去 leader 身份。
3. **幂等性**：重复 Put/Append 不能重复执行；Get 也不能在错误 leader 上“错误返回”。

因此 3A 的关键不是“把 KV 写到 map 里”，而是“**把 KV 请求正确地线性化**”。

### 4.2 功能设计（接口、错误码、幂等键）

#### 4.2.1 RPC 参数（ClientId/RequestId）

`(ClientId, RequestId)` 唯一标识一次用户请求：

- `ClientId`：客户端启动时随机生成（全局唯一概率极高）
- `RequestId`：单调递增序号，同一次 RPC 重试不改变

这保证了 Server 可以识别“同一个请求的重复投递”。

#### 4.2.2 错误码语义（指导 client 正确重试）

- `OK`：成功
- `ErrNoKey`：Get 的 key 不存在（语义正确返回）
- `ErrWrongLeader`：当前 server 不是 leader（或已失去 leader）
- `ErrTimeout`：等待 apply 超时（本次 RPC 不确定是否已被提交，必须重试并依赖去重保证幂等）

### 4.3 系统实现（关键数据结构与线程模型）

#### 4.3.1 KVServer 核心字段（实现“等待提交”与“去重”）

文件：`src/kvraft/server.go`

- `kvStore map[string]string`：状态机
- `lastApplied map[ClientId]RequestId`：去重表（每个客户端最新已处理序号）
- `lastResults map[ClientId]OpResult`：去重结果缓存（重复请求直接返回）
- `pendingReqs map[logIndex]*PendingRequest`：RPC handler 等待 apply 的桥梁

#### 4.3.2 “等待 Raft 提交”的实现策略（pendingReqs）

为什么需要 pendingReqs：

- `rf.Start()` 必须立即返回，不能阻塞到提交；
- 但对外 RPC 必须等到该操作真正被 commit/apply 才能回复（线性一致性）。

工程做法：

1. RPC handler 调用 `rf.Start(op)` 得到 `(index, term, isLeader)`
2. 若 `isLeader=false` 直接返回 `ErrWrongLeader`
3. 否则在 `pendingReqs[index]` 放入一个 `chan OpResult`
4. 等 applier apply 到该 index 后写回结果并唤醒
5. 为避免无限等待，引入 `ExecuteTimeout`（500ms）

#### 4.3.3 “回复前检查自己仍是 leader”（你强调的关键点）

我们在 `applyCommand()` 通知等待者时，做了 **term + isLeader 校验**：

- `currentTerm, isLeader := kv.rf.GetState()`
- 只有当 `isLeader && currentTerm == pending.term` 才返回真实结果
- 否则返回 `ErrWrongLeader`

这解决了典型风险：

> server 在 Start 后可能因网络分区/选举变为 follower；若仍返回成功，会让 client 误以为请求线性化到一个错误位置。

#### 4.3.4 去重（幂等）策略：把“不确定”变成“可重试”

网络不可靠下，client 可能永远收不到回复，此时只能重试。为了让重试安全：

- 对 Put/Append：
  - 若 `lastApplied[clientId] >= requestId`，直接返回上次结果（不重复执行写入）
- 对 Get：
  - Get 不改变状态，但仍通过 Raft 线性化（实现简单且安全）

#### 4.3.5 代码落地：executeOp / applier / applyCommand 的闭环（从 RPC 到状态机）

为了让“方法正确可落地”更直观，这里用接近源码的伪代码描述关键闭环（对应 `src/kvraft/server.go`）：

**(1) RPC handler → executeOp：提交到 Raft 并等待 apply**

```text
executeOp(op):
  1) 加锁：若 Put/Append 且 (clientId,reqId) 已处理 -> 直接返回缓存结果
  2) 解锁：rf.Start(op)
     - 非 leader：ErrWrongLeader
  3) 建立 pendingReqs[index] = {term, resultC}
  4) 等待 resultC 或超时(ExecuteTimeout)
     - 超时：ErrTimeout（client 必须重试，但 server 去重保证幂等）
```

**(2) Raft applier → KVServer.applyCommand：按序应用到状态机并唤醒等待者**

```text
applier goroutine:
  for msg := range applyCh:
    if msg.CommandValid:
      applyCommand(msg)

applyCommand(msg):
  1) 加锁：取出 op 与 index
  2) 若 Put/Append 且重复：直接使用 lastResults
     否则：对 kvStore 执行 Get/Put/Append 并更新 lastApplied/lastResults
  3) 如果 pendingReqs[index] 存在：
       term,isLeader := rf.GetState()
       if isLeader && term==pending.term: 返回真实结果
       else: 返回 ErrWrongLeader
       delete(pendingReqs[index])
  4) 解锁
```

**为什么这套闭环能同时满足：一致性 + 幂等 + 容错？**

- **一致性**：只有当操作被 Raft commit 并 apply 后才会对外返回成功，线性化点清晰。
- **幂等**：RPC 丢失导致的重试会被 `lastApplied/lastResults` 过滤。
- **leader 校验**：即使 server 在 Start 后失去 leader，也不会“误回复成功”，避免分区场景下的线性化错误。

### 4.4 Client 设计（重试、leader 缓存、无睡眠快速切换）

文件：`src/kvraft/client.go`

关键策略：

- 维护 `leaderId` 缓存（原子变量）
- RPC 失败或 `ErrWrongLeader/ErrTimeout` 时，轮询下一个 server
- 同一次操作的 `RequestId` 不变（保证 server 去重成立）

这保证了：

- 任何时候只要集群多数派可用，client 最终能找到 leader 并完成请求；
- 在丢包/超时下不会因为重试造成重复写入。

### 4.5 一致性与容错说明（把“正确性”讲清楚）

#### 4.5.1 线性一致性（Linearizability）

每个成功的 RPC 调用的线性化点是：

- 对 Put/Append/Get：对应的 `Op` 被 Raft commit 并由 KVServer apply 到状态机的时刻。

因为所有副本以相同顺序 apply 同一条 log，所以：

- 任意两个已完成操作在所有副本上观察到的顺序一致；
- Get 能看到最新 Put/Append 的结果（porcupine 检验通过）。

#### 4.5.2 容错（崩溃/分区/丢包）

- **丢包/超时**：client 重试 + server 去重，保证幂等
- **分区**：少数派不提交，client 在错误 leader 上会得到 ErrWrongLeader/Timeout，最终转向多数派 leader
- **重启**：Raft 2C 持久化恢复 log，KVServer 通过 applyCh 回放恢复一致状态

---

## 5. 测试结果与性能评估

> 说明：评分脚本通常不带 `-race`。此处给出我们在当前环境下的实际运行输出，用于对齐性能档位。

### 5.1 Lab 2（2A+2B+2C）测试与耗时（A 档）

命令：

```bash
cd src/raft
go test -run '2[ABC]' -count=1
```

输出（完整，通过与性能证明）：

```text
Test (2A): initial election ...
  ... Passed --   3.0  3   58   15720    0
Test (2A): election after network failure ...
  ... Passed --   4.5  3  136   25508    0
Test (2A): multiple elections ...
  ... Passed --   5.4  7  670  122372    0
Test (2B): basic agreement ...
  ... Passed --   0.5  3   16    4324    3
Test (2B): RPC byte count ...
  ... Passed --   1.3  3   48  113616   11
Test (2B): agreement after follower reconnects ...
  ... Passed --   5.4  3  136   35499    8
Test (2B): no agreement if too many followers disconnect ...
  ... Passed --   3.3  5  240   46480    3
Test (2B): concurrent Start()s ...
  ... Passed --   0.6  3   24    6791    6
Test (2B): rejoin of partitioned leader ...
  ... Passed --   6.0  3  196   46679    4
Test (2B): leader backs up quickly over incorrect follower logs ...
  ... Passed --  14.7  5 2140 1650167  102
Test (2B): RPC counts aren't too high ...
  ... Passed --   2.0  3   62   18446   12
Test (2C): basic persistence ...
  ... Passed --   3.3  3   90   22560    6
Test (2C): more persistence ...
  ... Passed --  14.7  5 1011  201663   16
Test (2C): partitioned leader and one follower crash, leader restarts ...
  ... Passed --   1.5  3   40   10113    4
Test (2C): Figure 8 ...
  ... Passed --  31.0  5 1580  331800   67
Test (2C): unreliable agreement ...
  ... Passed --   1.3  5 1040  340523  246
Test (2C): Figure 8 (unreliable) ...
  ... Passed --  35.7  5 10508 18839149   82
Test (2C): churn ...
  ... Passed --  16.1  5 15512 52413629 3697
Test (2C): unreliable churn ...
  ... Passed --  16.0  5 8276 12929233 1886
PASS
ok  	6.824/raft	166.524s
```

结论：**166.524s < 170s（A 档 12-15 分）**

### 5.2 Lab 3A 测试与耗时（A 档）

命令：

```bash
cd src/kvraft
go test -run 3A -count=1
```

输出（完整，通过与性能证明）：

```text
Test: one client (3A) ...
  ... Passed --  15.1  5 30794 5388
Test: ops complete fast enough (3A) ...
  ... Passed --   1.0  3  6153    0
Test: many clients (3A) ...
  ... Passed --  15.1  5 43970 5809
Test: unreliable net, many clients (3A) ...
  ... Passed --  15.7  5 12523 1815
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   0.7  3   214   52
Test: progress in majority (3A) ...
  ... Passed --   0.5  5    63    2
Test: no progress in minority (3A) ...
  ... Passed --   1.0  5   128    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    61    3
Test: partitions, one client (3A) ...
  ... Passed --  23.2  5 31725 5399
Test: partitions, many clients (3A) ...
  ... Passed --  22.4  5 55657 6543
Test: restarts, one client (3A) ...
  ... Passed --  18.8  5 38411 4680
Test: restarts, many clients (3A) ...
  ... Passed --  19.2  5 82464 5315
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  20.2  5 13785 1996
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  26.0  5 49125 4927
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  26.9  5 10500 1319
Test: unreliable net, restarts, partitions, random keys, many clients (3A) ...
  ... Passed --  29.1  7 34638 3228
PASS
ok  	6.824/kvraft	235.993s
```

结论：**235.993s < 250s（A 档 12-15 分）**

### 5.3 性能结论汇总表（直接对应评分标准）

| 指标 | 实测结果 | 档位 |
|---|---:|---|
| 2A+2B+2C 测试耗时 | 166.524s | A（<170s） |
| 3A 测试耗时 | 235.993s | A（<250s） |
| 功能测试通过率 | 100% | 满分项 |

---

## 6. 个人总结（困难、解决方案、心得体会）

### 6.1 困难与解决方案（按“现象→定位→修复→验证”结构）

#### 6.1.1 日志一致性与冲突修复边界（2B）

- **现象**：follower 日志与 leader 冲突时，可能出现无法推进 nextIndex、或反复回退导致 RPC 激增。
- **定位**：AppendEntries 的 PrevLogIndex/PrevLogTerm 检查与冲突截断逻辑的边界条件（日志过短 vs term 不匹配）。
- **修复**：严格实现冲突截断 + fast backup（XTerm/XIndex/XLen），并在 leader 侧用 term 跳跃回退。
- **验证**：2B `leader backs up quickly...` 与 `RPC counts aren't too high` 通过，且耗时/字节数在合理范围。

#### 6.1.2 Figure 8 提交规则（2C）

- **现象**：在分区/重启组合场景下，可能错误提交旧任期日志，导致 safety 被破坏（Figure 8 类问题）。
- **定位**：commitIndex 推进时缺少“只提交当前任期条目”的约束。
- **修复**：`advanceCommitIndex()` 中增加 `log[n].Term == currentTerm` 条件。
- **验证**：2C `Figure 8` 与 `Figure 8 (unreliable)` 通过。

#### 6.1.3 Client 重试导致的重复写入（3A）

- **现象**：网络丢包/超时下，client 重试可能导致 Put/Append 重复执行，破坏正确性。
- **定位**：server 未做幂等去重时，无法区分“新请求”与“同一请求的重试”。
- **修复**：引入 `(ClientId, RequestId)`，server 维护 `lastApplied + lastResults`，重复请求直接返回缓存结果。
- **验证**：3A 并发/不可靠/分区测试通过，porcupine 线性一致性检查通过。

#### 6.1.4 “回复前仍是 leader”的检查（3A）

- **现象**：server 在 Start 后可能失去 leader 身份；若仍返回成功会产生线性化错误。
- **修复**：pendingReqs 记录 `(index, term)`；apply 时校验 `isLeader && currentTerm==term`，否则返回 `ErrWrongLeader`。
- **验证**：3A 分区相关测试稳定通过。

#### 6.1.5 性能优化（对齐 A 档）

- **策略 1：选举超时区间**：将随机选举超时设为 `200~350ms`，减少 split vote，同时保证 5 秒内选主。
- **策略 2：立即复制**：Start/becomeLeader 后立即广播 AppendEntries，减少等待心跳 tick 的延迟。
- **策略 3：fast backup**：降低 churn/figure8 的回退 RPC 成本。
- **结果**：2A+2B+2C 166.524s；3A 235.993s，均为 A 档。

### 6.2 心得体会

1. **Raft 的难点在“边界与并发”**：论文规则不复杂，但实现必须对任期、日志索引、锁与 goroutine 的交织保持极度谨慎。
2. **KV 的难点在“RPC 不可靠 + 幂等”**：真正让系统可用的是“可重试且不重复执行”的工程闭环。
3. **测试即规格**：MIT 的测试覆盖了多数关键边界；通过失败用例反推缺陷是最高效的调试路径。

### 6.3 可改进方向（不影响本次提交范围）

1. **读优化（ReadIndex/lease）**：当前 Get 也走 Raft，简洁但会增加日志压力；可用 ReadIndex 优化（超出本次实验要求）。
2. **更细粒度锁与结构优化**：进一步降低 KVServer 热路径锁竞争。
3. **（不做 2D）日志空间控制策略**：本次明确不实现 2D 语义；如长期运行可考虑外部手段控制日志增长（本课程后续实验另行处理）。

---

## 附录：复现实验与代码索引

### A.1 一键复现命令

```bash
cd ~/distributed-sys/final-experiment/6.824/src/raft
go test -run '2[ABC]' -count=1

cd ../kvraft
go test -run 3A -count=1
```

### A.2 关键文件索引

- `src/raft/raft.go`：Raft（2A/2B/2C）核心逻辑 + 与测试兼容的接口定义
- `src/raft/persister.go`：持久化读写（2C）
- `src/kvraft/server.go`：KVServer（apply、去重、等待提交）
- `src/kvraft/client.go`：Clerk（重试与 leader 缓存）
- `src/kvraft/common.go`：RPC 结构与错误码

---

（完）


