# MIT 6.824 分布式系统实验报告
## 容错的Key-value键值存储系统

---

## 目录

1. [实验概述](#1-实验概述)
2. [Lab 2B: 日志复制与一致性](#2-lab-2b-日志复制与一致性)
3. [Lab 2C: 状态信息持久化](#3-lab-2c-状态信息持久化)
4. [Lab 3A: 基于Raft实现容错的key-value服务](#4-lab-3a-基于raft实现容错的key-value服务)
5. [测试结果](#5-测试结果)
6. [个人总结](#6-个人总结)

---

## 1. 实验概述

### 1.1 实验目标

本实验旨在构建一个基于Raft共识协议的容错键值存储系统。该系统具备以下特性：

- **容错性**：系统能够容忍部分节点失败，只要多数节点存活即可继续提供服务
- **强一致性**：通过Raft协议保证所有节点看到相同的操作顺序
- **持久化**：节点重启后能够恢复之前的状态
- **线性一致性**：客户端操作满足线性一致性要求

### 1.2 系统架构

```
+-------------+     +------------------+     +------------------+
|   Client    | --> |    KV Service    | --> |       Raft       |
|   (Clerk)   |     |    (KVServer)    |     |    (Consensus)   |
+-------------+     +------------------+     +------------------+
                            |                        |
                            v                        v
                    +---------------+        +---------------+
                    |  State Machine|        |  Replicated   |
                    |  (KV Store)   |        |     Log       |
                    +---------------+        +---------------+
```

### 1.3 技术栈

- 编程语言：Go
- RPC框架：labrpc（MIT提供的模拟RPC库）
- 序列化：labgob（Go的gob序列化的封装）

---

## 2. Lab 2B: 日志复制与一致性

### 2.1 任务分析

Lab 2B 的核心任务是实现 Raft 的日志复制机制，确保所有节点维护相同的日志副本。主要包括：

1. **日志条目结构设计**：定义日志条目的数据结构
2. **Start() 函数实现**：Leader接收客户端命令并追加到本地日志
3. **AppendEntries RPC**：实现日志同步和心跳机制
4. **日志一致性检查**：确保Follower的日志与Leader保持一致
5. **提交规则**：正确判断日志条目何时可以被提交
6. **Apply机制**：将已提交的日志应用到状态机

### 2.2 功能设计

#### 2.2.1 日志条目结构

```go
type LogEntry struct {
    Term    int         // 接收该条目时的任期号
    Command interface{} // 状态机命令
}
```

#### 2.2.2 Leader 状态

```go
// Volatile state on leaders (reinitialized after election)
nextIndex  []int // 对于每个服务器，下一个要发送的日志条目索引
matchIndex []int // 对于每个服务器，已知复制成功的最高日志索引
```

#### 2.2.3 AppendEntries RPC 设计

**请求参数：**
```go
type AppendEntriesArgs struct {
    Term         int        // Leader的任期
    LeaderId     int        // Leader的ID
    PrevLogIndex int        // 紧接新条目之前的日志索引
    PrevLogTerm  int        // PrevLogIndex对应的任期
    Entries      []LogEntry // 要追加的日志条目
    LeaderCommit int        // Leader的commitIndex
}
```

**响应参数：**
```go
type AppendEntriesReply struct {
    Term    int  // 当前任期，用于Leader更新自己
    Success bool // 如果Follower包含匹配的条目则为true
    
    // 快速回退优化
    XTerm   int  // 冲突条目的任期，-1表示日志过短
    XIndex  int  // XTerm的第一个索引
    XLen    int  // Follower日志长度
}
```

### 2.3 代码实现

#### 2.3.1 Start() 函数

```go
func (rf *Raft) Start(command interface{}) (int, int, bool) {
    rf.mu.Lock()
    defer rf.mu.Unlock()

    if rf.state != Leader {
        return -1, -1, false
    }

    // 追加条目到本地日志
    entry := LogEntry{
        Term:    rf.currentTerm,
        Command: command,
    }
    rf.log = append(rf.log, entry)
    rf.persist()

    index := rf.lastLogIndex()
    term := rf.currentTerm

    // 立即开始复制
    go rf.broadcastAppendEntries()

    return index, term, true
}
```

**设计要点：**
- 只有Leader才能接受新命令
- 命令首先追加到本地日志并持久化
- 立即触发日志复制，不等待心跳间隔

#### 2.3.2 AppendEntries 处理器

```go
func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) {
    rf.mu.Lock()
    defer rf.mu.Unlock()

    // 拒绝旧任期的请求
    if args.Term < rf.currentTerm {
        reply.Term = rf.currentTerm
        reply.Success = false
        return
    }

    // 更新任期并转为Follower
    if args.Term > rf.currentTerm {
        rf.currentTerm = args.Term
        rf.votedFor = -1
        rf.persist()
    }
    rf.state = Follower
    rf.lastHeard = time.Now()

    reply.Term = rf.currentTerm

    // 日志一致性检查
    if args.PrevLogIndex > 0 {
        if args.PrevLogIndex > rf.lastLogIndex() {
            // 日志过短
            reply.Success = false
            reply.XTerm = -1
            reply.XIndex = rf.lastLogIndex() + 1
            reply.XLen = len(rf.log)
            return
        }
        if rf.log[args.PrevLogIndex].Term != args.PrevLogTerm {
            // 发现冲突
            reply.Success = false
            reply.XTerm = rf.log[args.PrevLogIndex].Term
            reply.XIndex = args.PrevLogIndex
            for reply.XIndex > 1 && rf.log[reply.XIndex-1].Term == reply.XTerm {
                reply.XIndex--
            }
            reply.XLen = len(rf.log)
            return
        }
    }

    // 追加日志条目
    for i, entry := range args.Entries {
        idx := args.PrevLogIndex + 1 + i
        if idx < len(rf.log) {
            if rf.log[idx].Term != entry.Term {
                // 冲突：删除该条目及之后的所有条目
                rf.log = rf.log[:idx]
                rf.log = append(rf.log, args.Entries[i:]...)
                rf.persist()
                break
            }
        } else {
            rf.log = append(rf.log, args.Entries[i:]...)
            rf.persist()
            break
        }
    }

    // 更新commitIndex
    if args.LeaderCommit > rf.commitIndex {
        if args.LeaderCommit < rf.lastLogIndex() {
            rf.commitIndex = args.LeaderCommit
        } else {
            rf.commitIndex = rf.lastLogIndex()
        }
        rf.applyCond.Signal()
    }

    reply.Success = true
}
```

#### 2.3.3 快速回退优化

当Follower的日志与Leader不一致时，传统方法是每次递减nextIndex一步，效率较低。本实现采用了快速回退优化：

```go
if reply.XTerm == -1 {
    // 日志过短，直接跳到日志末尾
    rf.nextIndex[server] = reply.XLen
} else {
    // 查找Leader是否有XTerm的条目
    found := false
    for idx := args.PrevLogIndex; idx >= 1; idx-- {
        if rf.log[idx].Term == reply.XTerm {
            rf.nextIndex[server] = idx + 1
            found = true
            break
        }
        if rf.log[idx].Term < reply.XTerm {
            break
        }
    }
    if !found {
        rf.nextIndex[server] = reply.XIndex
    }
}
```

#### 2.3.4 提交规则实现

```go
func (rf *Raft) advanceCommitIndex() {
    for n := rf.lastLogIndex(); n > rf.commitIndex; n-- {
        // 只提交当前任期的条目
        if rf.log[n].Term != rf.currentTerm {
            continue
        }

        count := 1 // 计入自己
        for i := range rf.peers {
            if i != rf.me && rf.matchIndex[i] >= n {
                count++
            }
        }

        if count > len(rf.peers)/2 {
            rf.commitIndex = n
            rf.applyCond.Signal()
            break
        }
    }
}
```

**关键设计决策：**
- 只提交当前任期的日志条目（Figure 8 问题）
- 使用多数派规则确定提交点

---

## 3. Lab 2C: 状态信息持久化

### 3.1 任务分析

Lab 2C 的核心任务是实现Raft状态的持久化，确保节点重启后能够恢复状态。需要持久化的状态包括：

1. **currentTerm**：当前任期号
2. **votedFor**：投票给的候选人ID
3. **log[]**：日志条目数组

### 3.2 功能设计

#### 3.2.1 持久化时机

需要在以下场景进行持久化：
- 任期号增加时
- 投票给某候选人时
- 日志发生变化时

#### 3.2.2 序列化格式

使用labgob编码器将状态序列化为字节数组：

```go
func (rf *Raft) persist() {
    w := new(bytes.Buffer)
    e := labgob.NewEncoder(w)
    e.Encode(rf.currentTerm)
    e.Encode(rf.votedFor)
    e.Encode(rf.log)
    data := w.Bytes()
    rf.persister.SaveRaftState(data)
}
```

### 3.3 代码实现

#### 3.3.1 persist() 函数

```go
func (rf *Raft) persist() {
    w := new(bytes.Buffer)
    e := labgob.NewEncoder(w)
    e.Encode(rf.currentTerm)
    e.Encode(rf.votedFor)
    e.Encode(rf.log)
    data := w.Bytes()
    rf.persister.SaveRaftState(data)
}
```

#### 3.3.2 readPersist() 函数

```go
func (rf *Raft) readPersist(data []byte) {
    if data == nil || len(data) < 1 {
        return
    }
    r := bytes.NewBuffer(data)
    d := labgob.NewDecoder(r)
    var currentTerm int
    var votedFor int
    var log []LogEntry
    if d.Decode(&currentTerm) != nil ||
        d.Decode(&votedFor) != nil ||
        d.Decode(&log) != nil {
        return
    }
    rf.currentTerm = currentTerm
    rf.votedFor = votedFor
    rf.log = log
}
```

#### 3.3.3 持久化调用点

在代码中的以下位置调用 `persist()`：

1. **RequestVote处理器中**：
   - 更新任期号时
   - 投票给候选人时

2. **AppendEntries处理器中**：
   - 更新任期号时
   - 追加日志条目时

3. **选举过程中**：
   - 开始新选举时（增加任期，投票给自己）

4. **Start()函数中**：
   - 追加新日志条目时

---

## 4. Lab 3A: 基于Raft实现容错的key-value服务

### 4.1 任务分析

Lab 3A 的核心任务是在Raft之上构建一个容错的键值存储服务。主要包括：

1. **Clerk（客户端）**：实现Get/Put/Append操作的RPC发送和重试逻辑
2. **KVServer（服务端）**：实现请求处理、与Raft层交互、状态机应用
3. **重复请求检测**：确保每个请求只执行一次
4. **线性一致性**：保证操作的线性一致性

### 4.2 功能设计

#### 4.2.1 操作定义

```go
type Op struct {
    Type      string // "Get", "Put", or "Append"
    Key       string
    Value     string
    ClientId  int64
    RequestId int64
}
```

#### 4.2.2 重复检测机制

使用 (ClientId, RequestId) 对唯一标识每个请求：
- ClientId：客户端唯一标识（启动时随机生成）
- RequestId：单调递增的序列号

服务端维护每个客户端已处理的最大RequestId，用于检测重复请求。

#### 4.2.3 处理流程

```
1. Client --> KVServer: 发送RPC请求 (Get/Put/Append)
2. KVServer: 检查是否为重复请求
3. KVServer --> Raft: 调用Start()提交命令
4. Raft: 执行日志复制，达成共识
5. Raft --> KVServer: 通过applyCh通知已提交
6. KVServer: 应用命令到状态机
7. KVServer --> Client: 返回结果
```

### 4.3 代码实现

#### 4.3.1 KVServer 结构

```go
type KVServer struct {
    mu      sync.Mutex
    me      int
    rf      *raft.Raft
    applyCh chan raft.ApplyMsg
    dead    int32

    maxraftstate int

    // 键值存储
    kvStore map[string]string

    // 重复检测：记录每个客户端的最后请求ID
    lastApplied map[int64]int64

    // 缓存每个客户端最后请求的结果
    lastResults map[int64]OpResult

    // 等待Raft提交的请求
    pendingReqs map[int]*PendingRequest
}
```

#### 4.3.2 请求处理

```go
func (kv *KVServer) executeOp(op Op) OpResult {
    kv.mu.Lock()

    // 检查重复请求（仅针对Put/Append）
    if op.Type != "Get" {
        if lastReqId, ok := kv.lastApplied[op.ClientId]; ok && lastReqId >= op.RequestId {
            result := kv.lastResults[op.ClientId]
            kv.mu.Unlock()
            return result
        }
    }
    kv.mu.Unlock()

    // 提交到Raft
    index, term, isLeader := kv.rf.Start(op)
    if !isLeader {
        return OpResult{Err: ErrWrongLeader}
    }

    // 创建等待通道
    resultC := make(chan OpResult, 1)
    kv.mu.Lock()
    kv.pendingReqs[index] = &PendingRequest{
        index:   index,
        term:    term,
        resultC: resultC,
    }
    kv.mu.Unlock()

    // 等待结果或超时
    select {
    case result := <-resultC:
        return result
    case <-time.After(ExecuteTimeout):
        kv.mu.Lock()
        delete(kv.pendingReqs, index)
        kv.mu.Unlock()
        return OpResult{Err: ErrTimeout}
    }
}
```

#### 4.3.3 Apply 处理

```go
func (kv *KVServer) applyCommand(msg raft.ApplyMsg) {
    kv.mu.Lock()
    defer kv.mu.Unlock()

    op := msg.Command.(Op)
    index := msg.CommandIndex

    var result OpResult

    // 重复检测
    if op.Type != "Get" {
        if lastReqId, ok := kv.lastApplied[op.ClientId]; ok && lastReqId >= op.RequestId {
            // 重复请求，使用缓存结果
            result = kv.lastResults[op.ClientId]
        } else {
            // 执行操作
            result = kv.applyOp(op)
            kv.lastApplied[op.ClientId] = op.RequestId
            kv.lastResults[op.ClientId] = result
        }
    } else {
        // Get请求总是执行
        result = kv.applyOp(op)
    }

    // 通知等待的请求
    if pending, ok := kv.pendingReqs[index]; ok {
        currentTerm, isLeader := kv.rf.GetState()
        if isLeader && currentTerm == pending.term {
            pending.resultC <- result
        } else {
            pending.resultC <- OpResult{Err: ErrWrongLeader}
        }
        delete(kv.pendingReqs, index)
    }
}
```

#### 4.3.4 Clerk 实现

```go
func (ck *Clerk) Get(key string) string {
    requestId := atomic.AddInt64(&ck.seqNum, 1)
    args := GetArgs{
        Key:       key,
        ClientId:  ck.clientId,
        RequestId: requestId,
    }

    for {
        leaderId := atomic.LoadInt32(&ck.leaderId)
        reply := GetReply{}
        ok := ck.servers[leaderId].Call("KVServer.Get", &args, &reply)

        if ok {
            switch reply.Err {
            case OK:
                return reply.Value
            case ErrNoKey:
                return ""
            case ErrWrongLeader, ErrTimeout:
                // 尝试下一个服务器
            }
        }

        // 尝试下一个服务器
        atomic.StoreInt32(&ck.leaderId, (leaderId+1)%int32(len(ck.servers)))
    }
}
```

**设计要点：**
- 缓存上次成功的Leader ID，减少不必要的重试
- 使用原子操作保证线程安全
- 无限重试直到成功

---

## 5. 测试结果

### 5.1 Lab 2 测试结果

#### Lab 2A (Leader Election)

```
Test (2A): initial election ...
  ... Passed --   3.1  3   62   16580    0
Test (2A): election after network failure ...
  ... Passed --   4.5  3  142   26770    0
Test (2A): multiple elections ...
  ... Passed --   5.4  7  734  133666    0
```

#### Lab 2B (Log Replication)

```
Test (2B): basic agreement ...
  ... Passed --   0.6  3   18    4874    3
Test (2B): RPC byte count ...
  ... Passed --   1.4  3   52  114476   11
Test (2B): agreement after follower reconnects ...
  ... Passed --   3.4  3   94   23483    7
Test (2B): no agreement if too many followers disconnect ...
  ... Passed --   3.4  5  232   46272    3
Test (2B): concurrent Start()s ...
  ... Passed --   0.5  3   20    5748    6
Test (2B): rejoin of partitioned leader ...
  ... Passed --   4.0  3  158   35838    4
Test (2B): leader backs up quickly over incorrect follower logs ...
  ... Passed --  16.7  5 2219 1670345  103
Test (2B): RPC counts aren't too high ...
  ... Passed --   2.1  3   64   19114   12
```

#### Lab 2C (Persistence)

```
Test (2C): basic persistence ...
  ... Passed --   3.2  3   88   21850    6
Test (2C): more persistence ...
  ... Passed --  14.7  5  974  199868   16
Test (2C): partitioned leader and one follower crash, leader restarts ...
  ... Passed --   1.5  3   40    9903    4
Test (2C): Figure 8 ...
  ... Passed --  27.4  5 1432  308306   57
Test (2C): unreliable agreement ...
  ... Passed --   1.4  5 1044  344937  246
Test (2C): Figure 8 (unreliable) ...
  ... Passed --  27.9  5 10848 43658778  290
Test (2C): churn ...
  ... Passed --  16.1  5 11180 39915711 2613
Test (2C): unreliable churn ...
  ... Passed --  16.2  5 8688 5951670 1970
```

**Lab 2 总耗时：153.6 秒** ✓ (A档 < 170s)

### 5.2 Lab 3A 测试结果

```
Test: one client (3A) ...
  ... Passed --  15.1  5 24343 4115
Test: ops complete fast enough (3A) ...
  ... Passed --   1.4  3  5555    0
Test: many clients (3A) ...
  ... Passed --  16.3  5 36562 4798
Test: unreliable net, many clients (3A) ...
  ... Passed --  15.9  5 12585 1862
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   1.0  3   250   52
Test: progress in majority (3A) ...
  ... Passed --   0.3  5    61    2
Test: no progress in minority (3A) ...
  ... Passed --   1.0  5   125    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    62    3
Test: partitions, one client (3A) ...
  ... Passed --  22.4  5 26580 4170
Test: partitions, many clients (3A) ...
  ... Passed --  22.7  5 56044 5424
Test: restarts, one client (3A) ...
  ... Passed --  19.0  5 29447 3652
Test: restarts, many clients (3A) ...
  ... Passed --  19.0  5 71028 4811
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  20.5  5 13203 1829
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  26.4  5 56515 5316
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  27.8  5 10510 1232
Test: unreliable net, restarts, partitions, random keys, many clients (3A) ...
  ... Passed --  29.1  7 36275 3258
```

**Lab 3A 总耗时：239.2 秒** ✓ (A档 < 250s)

### 5.3 性能评估

| 指标 | 结果 | 评级 |
|------|------|------|
| Lab 2A+2B+2C 耗时 | 153.6s | A档 (< 170s) |
| Lab 3A 耗时 | 239.2s | A档 (< 250s) |
| 所有测试通过率 | 100% | 完全通过 |

---

## 6. 个人总结

### 6.1 遇到的困难及解决方案

#### 6.1.1 日志一致性问题

**困难**：在实现 AppendEntries 时，如何正确处理日志不一致的情况，特别是当Follower的日志与Leader冲突时。

**解决方案**：
1. 严格按照Raft论文Figure 2的规范实现
2. 实现了快速回退优化（XTerm/XIndex/XLen），大幅减少了不一致时的RPC次数
3. 仔细处理日志截断和追加的边界条件

#### 6.1.2 Figure 8 问题

**困难**：理解为什么Leader不能直接提交之前任期的日志条目。

**解决方案**：
1. 深入理解Raft论文5.4.2节的内容
2. 在 `advanceCommitIndex()` 中添加检查，只提交当前任期的条目
3. 通过提交当前任期的新条目来间接提交旧条目

#### 6.1.3 重复请求检测

**困难**：如何在分布式环境下保证每个请求只执行一次（幂等性）。

**解决方案**：
1. 使用 (ClientId, RequestId) 唯一标识每个请求
2. 服务端维护每个客户端的最后请求ID
3. 缓存最后一个请求的结果，用于重复请求的快速响应

#### 6.1.4 死锁问题

**困难**：在实现过程中多次遇到死锁，特别是在Raft和KVServer的交互中。

**解决方案**：
1. 使用条件变量（sync.Cond）替代channel进行线程间通信
2. 确保锁的获取顺序一致
3. 避免在持有锁时进行RPC调用

#### 6.1.5 性能优化

**困难**：初始版本的测试耗时较长，无法达到A档标准。

**解决方案**：
1. 优化选举超时时间范围（250-400ms）
2. 在收到新命令时立即触发日志复制，不等待心跳
3. 使用批量发送减少RPC次数

### 6.2 心得体会

1. **理论与实践的结合**：通过这个实验，深刻理解了Raft协议的设计思想和实现细节。论文中看似简单的规则，在实现时需要考虑很多边界情况。

2. **并发编程的挑战**：分布式系统本质上是并发系统，需要仔细处理竞态条件、死锁等问题。Go的goroutine和channel虽然简化了并发编程，但仍需谨慎使用。

3. **测试驱动开发**：MIT提供的测试用例覆盖了各种边界情况和故障场景，通过测试来发现和修复问题是非常有效的方法。

4. **调试技巧**：分布式系统的调试比单机程序困难得多，需要合理使用日志输出和状态检查。

5. **代码质量**：良好的代码结构和清晰的注释对于理解和维护分布式系统代码至关重要。

### 6.3 改进方向

1. **快照支持**：当前实现不包含Lab 2D的快照功能，日志会无限增长
2. **更细粒度的锁**：可以考虑使用读写锁或更细粒度的锁来提高并发性能
3. **日志压缩**：可以实现日志压缩来减少内存和存储开销

---

## 附录：核心代码文件

- `src/raft/raft.go`：Raft共识协议实现
- `src/kvraft/server.go`：KVServer服务端实现
- `src/kvraft/client.go`：Clerk客户端实现
- `src/kvraft/common.go`：公共数据结构定义

---

*报告编写日期：2024年*

*MIT 6.824 分布式系统实验*